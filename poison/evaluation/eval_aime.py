import os
import json
import argparse
import re
from typing import List, Dict, Optional
import numpy as np

def extract_answer_from_response(response: str) -> Optional[str]:
    """
    ä»æ¨¡å‹å“åº”ä¸­æå–æ•°å€¼ç­”æ¡ˆ
    AIMEç­”æ¡ˆé€šå¸¸æ˜¯0-999ä¹‹é—´çš„æ•´æ•°
    """
    # ç§»é™¤thinkingæ ‡ç­¾å†…å®¹
    if "</think>" in response:
        response = response.split("</think>", 1)[1].strip()
    
    # å¯»æ‰¾å¸¸è§çš„ç­”æ¡ˆæ¨¡å¼
    patterns = [
        r'ç­”æ¡ˆæ˜¯\s*(\d{1,3})',
        r'ç­”æ¡ˆä¸º\s*(\d{1,3})',
        r'Therefore,?\s*the answer is\s*(\d{1,3})',
        r'So,?\s*the answer is\s*(\d{1,3})',
        r'The answer is\s*(\d{1,3})',
        r'\\boxed\{(\d{1,3})\}',
        r'ç­”æ¡ˆï¼š\s*(\d{1,3})',
        r'æœ€ç»ˆç­”æ¡ˆæ˜¯\s*(\d{1,3})',
        r'æœ€ç»ˆç­”æ¡ˆä¸º\s*(\d{1,3})',
        r'Final answer:\s*(\d{1,3})',
        r'Answer:\s*(\d{1,3})',
        r'ç»“æœæ˜¯\s*(\d{1,3})',
        r'ç»“æœä¸º\s*(\d{1,3})',
        r'æ‰€ä»¥ç­”æ¡ˆæ˜¯\s*(\d{1,3})',
        r'å› æ­¤ç­”æ¡ˆæ˜¯\s*(\d{1,3})',
        r'æ•…ç­”æ¡ˆä¸º\s*(\d{1,3})',
        r'=\s*(\d{1,3})\s*$',  # ä»¥ç­‰å·ç»“å°¾çš„æ•°å­—
        r'\b(\d{1,3})\s*$',    # ä»¥æ•°å­—ç»“å°¾
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, response, re.IGNORECASE | re.MULTILINE)
        if matches:
            answer = matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…
            try:
                num = int(answer)
                if 0 <= num <= 999:  # AIMEç­”æ¡ˆèŒƒå›´
                    return str(num)
            except ValueError:
                continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå°è¯•æå–æ‰€æœ‰3ä½ä»¥å†…çš„æ•°å­—ï¼Œå–æœ€åä¸€ä¸ª
    numbers = re.findall(r'\b(\d{1,3})\b', response)
    if numbers:
        for num_str in reversed(numbers):
            try:
                num = int(num_str)
                if 0 <= num <= 999:
                    return str(num)
            except ValueError:
                continue
    
    return None

def load_ground_truth(dataset_name: str) -> Dict[str, str]:
    """
    åŠ è½½æ­£ç¡®ç­”æ¡ˆ
    """
    ground_truth = {}
    
    if "aime24" in dataset_name.lower():
        # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ AIME24 æ•°æ®
        aime24_path = "/mnt/workspace/wzn/AIME/aime24.jsonl"
        
        try:
            with open(aime24_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        example = json.loads(line)
                        # å¤„ç†å¯èƒ½çš„ä¸åŒå­—æ®µå
                        problem = example.get("Problem", example.get("problem", ""))
                        answer = str(example.get("Answer", example.get("answer", "")))
                        if problem and answer:
                            ground_truth[problem] = answer
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {aime24_path}")
            return {}
        except Exception as e:
            print(f"é”™è¯¯: è¯»å–æ–‡ä»¶ {aime24_path} æ—¶å‡ºç°é—®é¢˜: {e}")
            return {}
            
    elif "aime25" in dataset_name.lower():
        # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ AIME25 æ•°æ®
        aime25_path = "/mnt/workspace/wzn/AIME/aime25.jsonl"
        
        try:
            with open(aime25_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        example = json.loads(line)
                        # å¤„ç†å¯èƒ½çš„ä¸åŒå­—æ®µå
                        problem = example.get("problem", example.get("Problem", ""))
                        answer = str(example.get("answer", example.get("Answer", "")))
                        if problem and answer:
                            ground_truth[problem] = answer
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {aime25_path}")
            return {}
        except Exception as e:
            print(f"é”™è¯¯: è¯»å–æ–‡ä»¶ {aime25_path} æ—¶å‡ºç°é—®é¢˜: {e}")
            return {}
    
    return ground_truth

def evaluate_predictions(input_path: str, dataset_name: str, verbose: bool = False) -> Dict:
    """
    è¯„ä¼°é¢„æµ‹ç»“æœ
    """
    # åŠ è½½é¢„æµ‹ç»“æœ
    with open(input_path, 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    
    # åŠ è½½æ­£ç¡®ç­”æ¡ˆ
    ground_truth = load_ground_truth(dataset_name)
    
    # è¯„ä¼°æŒ‡æ ‡
    total_problems = len(predictions)
    correct_predictions = 0
    extracted_answers = 0
    evaluation_results = []
    
    for pred in predictions:
        instruction = pred['instruction']
        model_output = pred.get('output', pred.get('raw_output', ''))
        
        # æå–æ¨¡å‹ç­”æ¡ˆ
        predicted_answer = extract_answer_from_response(model_output)
        
        # è·å–æ­£ç¡®ç­”æ¡ˆ
        true_answer = ground_truth.get(instruction)
        
        # è¯„ä¼°ç»“æœ
        result = {
            'instruction': instruction,
            'predicted_answer': predicted_answer,
            'true_answer': true_answer,
            'model_output': model_output,
            'correct': False,
            'answer_extracted': predicted_answer is not None
        }
        
        if predicted_answer is not None:
            extracted_answers += 1
            if true_answer is not None and predicted_answer == true_answer:
                correct_predictions += 1
                result['correct'] = True
        
        evaluation_results.append(result)
        
        if verbose:
            status = "âœ“" if result['correct'] else "âœ—"
            extract_status = "ğŸ“" if result['answer_extracted'] else "âŒ"
            print(f"{status} {extract_status} Pred: {predicted_answer} | True: {true_answer}")
            if len(instruction) > 100:
                print(f"   Problem: {instruction[:100]}...")
            else:
                print(f"   Problem: {instruction}")
            print()
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = correct_predictions / total_problems if total_problems > 0 else 0
    extraction_rate = extracted_answers / total_problems if total_problems > 0 else 0
    accuracy_on_extracted = correct_predictions / extracted_answers if extracted_answers > 0 else 0
    
    metrics = {
        'total_problems': total_problems,
        'correct_predictions': correct_predictions,
        'extracted_answers': extracted_answers,
        'accuracy': accuracy,
        'extraction_rate': extraction_rate,
        'accuracy_on_extracted': accuracy_on_extracted,
        'evaluation_results': evaluation_results
    }
    
    return metrics

def print_evaluation_summary(metrics: Dict, dataset_name: str):
    """
    æ‰“å°è¯„ä¼°æ‘˜è¦
    """
    print("=" * 60)
    print(f"AIME è¯„ä¼°ç»“æœæ€»ç»“ - {dataset_name}")
    print("=" * 60)
    print(f"æ€»é¢˜ç›®æ•°é‡: {metrics['total_problems']}")
    print(f"æˆåŠŸæå–ç­”æ¡ˆçš„é¢˜ç›®æ•°é‡: {metrics['extracted_answers']}")
    print(f"æ­£ç¡®é¢„æµ‹çš„é¢˜ç›®æ•°é‡: {metrics['correct_predictions']}")
    print()
    print(f"æ•´ä½“å‡†ç¡®ç‡: {metrics['accuracy']:.2%} ({metrics['correct_predictions']}/{metrics['total_problems']})")
    print(f"ç­”æ¡ˆæå–ç‡: {metrics['extraction_rate']:.2%} ({metrics['extracted_answers']}/{metrics['total_problems']})")
    print(f"æå–ç­”æ¡ˆä¸­çš„å‡†ç¡®ç‡: {metrics['accuracy_on_extracted']:.2%} ({metrics['correct_predictions']}/{metrics['extracted_answers']})")
    print("=" * 60)

def analyze_errors(metrics: Dict, top_k: int = 5):
    """
    åˆ†æé”™è¯¯æ¡ˆä¾‹
    """
    print(f"\né”™è¯¯æ¡ˆä¾‹åˆ†æ (æ˜¾ç¤ºå‰ {top_k} ä¸ª):")
    print("-" * 60)
    
    error_cases = [r for r in metrics['evaluation_results'] if not r['correct']]
    
    for i, case in enumerate(error_cases[:top_k]):
        print(f"\né”™è¯¯æ¡ˆä¾‹ {i+1}:")
        print(f"é—®é¢˜: {case['instruction'][:150]}...")
        print(f"é¢„æµ‹ç­”æ¡ˆ: {case['predicted_answer']}")
        print(f"æ­£ç¡®ç­”æ¡ˆ: {case['true_answer']}")
        print(f"ç­”æ¡ˆæ˜¯å¦æå–æˆåŠŸ: {'æ˜¯' if case['answer_extracted'] else 'å¦'}")
        
        if case['model_output']:
            print(f"æ¨¡å‹è¾“å‡º: {case['model_output'][:200]}...")
        print("-" * 40)

def save_detailed_results(metrics: Dict, output_file: str):
    """
    ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    """
    detailed_results = {
        'summary': {
            'total_problems': metrics['total_problems'],
            'correct_predictions': metrics['correct_predictions'],
            'extracted_answers': metrics['extracted_answers'],
            'accuracy': metrics['accuracy'],
            'extraction_rate': metrics['extraction_rate'],
            'accuracy_on_extracted': metrics['accuracy_on_extracted']
        },
        'detailed_results': metrics['evaluation_results']
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nè¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°AIMEæ•°æ®é›†ä¸Šçš„æ¨¡å‹é¢„æµ‹ç»“æœ")
    parser.add_argument("--input_path", required=True, help="pred_adaptive.pyç”Ÿæˆçš„é¢„æµ‹ç»“æœæ–‡ä»¶")
    parser.add_argument("--dataset_name", choices=['aime24', 'aime25'], default='aime25',
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--output_file", default="", help="è¯¦ç»†è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„é€é¢˜è¯„ä¼°ç»“æœ")
    parser.add_argument("--analyze_errors", type=int, default=5, help="åˆ†æé”™è¯¯æ¡ˆä¾‹çš„æ•°é‡")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_path):
        print(f"é”™è¯¯: é¢„æµ‹æ–‡ä»¶ {args.input_path} ä¸å­˜åœ¨")
        return
    
    print(f"æ­£åœ¨è¯„ä¼°æ–‡ä»¶: {args.input_path}")
    print(f"ä½¿ç”¨æ•°æ®é›†: {args.dataset_name}")
    print()
    
    # æ‰§è¡Œè¯„ä¼°
    metrics = evaluate_predictions(args.input_path, args.dataset_name, args.verbose)
    
    # æ‰“å°æ€»ç»“
    print_evaluation_summary(metrics, args.dataset_name)
    
    # åˆ†æé”™è¯¯æ¡ˆä¾‹
    if args.analyze_errors > 0:
        analyze_errors(metrics, args.analyze_errors)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if args.output_file:
        save_detailed_results(metrics, args.output_file)
    elif args.input_path.endswith('.json'):
        default_output = args.input_path.replace('.json', '_evaluation.json')
        save_detailed_results(metrics, default_output)

if __name__ == "__main__":
    main() 